\documentclass[11pt, a4paper, scheme=plain]{ctexart} 
% 'scheme=plain' keeps headings in English style (Section 1) 
% while 'ctexart' ensures Chinese characters render correctly.

% --- Basic Packages ---
\usepackage{geometry}     % Page layout
\usepackage{graphicx}     % Images
\usepackage{float}        % Force image placement (H)
\usepackage{amsmath}      % Math formulas
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xcolor}       % Colors
\usepackage{listings}     % Code blocks
\usepackage{hyperref}     % Hyperlinks
\usepackage{enumitem}     % Better lists
\usepackage{booktabs} % 用于绘制更专业的表格横线
\usepackage{siunitx}  % 用于对齐小数点
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\let\bf\mathbf
% --- Layout Settings ---
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\setlength{\parindent}{0pt}   % No indentation for paragraphs (better for logs)
\setlength{\parskip}{0.8em}   % Space between paragraphs

% --- Code Block Style ---
\definecolor{bg}{rgb}{0.96,0.96,0.96}
\lstset{
    backgroundcolor=\color{bg},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    columns=fullflexible,
    keepspaces=true
}

% --- Custom Commands for Logging ---
% 1. Timestamp: Usage \timecheck{14:30}
\newcommand{\timecheck}[1]{\textbf{\textcolor{blue}{[#1]}}}

% 2. Inline Code: Usage \code{filename.py}
\newcommand{\code}[1]{\colorbox{bg}{\texttt{#1}}}

% --- Metadata ---
\title{\textbf{Element of Statistical Learning Note}}
\author{Zichong Wang}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\clearpage

% --- Section 1: Setup ---
\section{Chap 3: Linear Methods for Regression}

\subsection{Confidence region for \texorpdfstring{$\hat{\beta}$}{beta hat}}

We know that $\hat \beta \sim \mathcal N (\beta, \sigma^2 (\bm X^T\bm X)^{-1})$,
thus $\hat \beta - \beta \sim \mathcal N (0, \sigma^2 (\bm X^T\bm X)^{-1})$.

Since for $\bm z \sim \mathcal N (0, \bm \Sigma)$, we have $\bm z^T \bm \Sigma^{-1} \bm z \sim \chi^2_k$ , where $k = \text{rank}(\bm \Sigma)$,
we have 

$$(\hat \beta - \beta)^T (\sigma^2 (\bm X^T\bm X)^{-1})^{-1} (\hat \beta - \beta) \sim \chi^2_{p+1}$$

Thus, 

$$ \frac{1}{\sigma^2} (\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) \sim \chi^2_{p+1} $$

And have approx confidence region

$$(\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) \le \hat{\sigma}^2 \chi^2_{p+1, 1-\alpha} $$

Actually, $\hat \sigma^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2 = \frac{1}{N-p-1} \text{RSS}$, and $\text{RSS}/\sigma^2 \sim \chi^2_{N-p-1}$,

$$ \frac{(\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) / (p+1)}{\hat{\sigma}^2} \sim F_{p+1, N-p-1} $$

\subsection{What is Linear}

In the context of \textbf{linear model}, we are talking about linearity in parameters, meaning that the prediction $\hat{y}$ is a linear combination of the parameters $\beta_j$.
The $\bm X$ itself can be non-linear transformations of the original features, e.g., polynomial terms, interaction terms, etc.
$y = 1 / (\beta_0 + \beta_1 x)$ and $y = \beta_0 e^{\beta_1 x}$ are not linear models, since they're not linear in parameters.

In the context of \textbf{Linear estimators}, we are talking about the estimator $\hat \theta$ (e.g. $\hat \beta$) can be written as 
a linear combination of the observed response values $y_i$, i.e. $\hat \theta = \bm c^T \bm y$. The weight $\bm c$ depends only on $\bm X$, not on $\bm y$.
A linear estimator \textbf{can be} a prediction at a new point, or the estimated coefficients $\hat \beta$ themselves.

\subsection{Gauss-Markov Theorem}

Why assume only know $\bm X$, but not $\bm y$?

\begin{quote}
     Note that though $y_i$ as sample responses, are observable, 
     the following statements and arguments including assumptions, proofs and the others assume under the only condition of knowing $\bm X_{i,j}$ but not $y_i$. \hfill --- \cite{wiki_gauss}
\end{quote}

We have a \textit{challenger} linear estimator $\tilde{\beta} = \bm C \bm y$, where $\bm C = (\bm X^T \bm X)^{-1} \bm X^T + \bm D$, a modification of OLS estimator.
Ensure it's unbiased:
\begin{align*}
    \mathbb E(\tilde{\beta}) &= \mathbb{E}(\bm C \bm y) \\
    &= \bm C \mathbb E(\bm y) = ((\bm X^T \bm X)^{-1} \bm X^T + \bm D) \bm X \beta \\
    &= \beta + \bm D \bm X \beta\\
    &= \beta
\end{align*}
Meaning $\bm D \bm X = 0$.

Now, compute the variance:
\begin{align*}
    \text{Var}(\tilde{\beta}) &= \text{Var}(\bm C \bm y) \\
    &= \bm C \text{Var}(\bm y) \bm C^T \\
    &= \sigma^2 \bm C \bm C^T \\
    &= \sigma^2 ((\bm X^T \bm X)^{-1} \bm X^T + \bm D) ((\bm X^T \bm X)^{-1} \bm X^T + \bm D)^T \\
    &= \sigma^2 ((\bm X^T \bm X)^{-1} + \bm D \bm D^T)\\
    &= \text{Var}(\hat{\beta}) + \sigma^2 \bm D \bm D^T\\
    &\ge \text{Var}(\hat{\beta})
\end{align*}

\subsection{QR decomposition}

Any real squared matrix $\bm A$ can be decomposed as $\bm A = \bm Q \bm R$, where $\bm Q$ is orthogonal matrix ($\bm Q^T \bm Q = \bm I$), and $\bm R$ is upper triangular matrix.

Any rectangular matrix $\bm A \in \mathbb R^{m \times n}$ ($m \ge n$), we can decompose it as $\bm A = \bm Q \bm R$, where $\bm Q$ is $m \times m$ orthogonal matrix, and $\bm R$ is $m \times n$ upper triangular matrix (the last $m-n$ rows are all zero).
It can be regarded as $\bm A = \bm Q \bm R=[\bm Q_1\quad \bm Q_2]\begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix} = \bm{Q}_1\bm{R}_1,$
$\text{where } \bm{Q}_1 \in \mathbb{R}^{m \times n}, \bm{Q}_2 \in \mathbb{R}^{m \times (m-n)}, \bm{R}_1 \in \mathbb{R}^{n \times n} \text{ which is upper triangular}$.

If $\bm A$ have $k$ linearly independent columns, then first $k$ columns of $\bm Q$ form an orthonormal basis of the column space of $\bm A$.
The fact that any column $k$ of $\bm A$ only depends on the first $k$ columns of $\bm Q$ corresponds to the triangular form of $\bm R$.


QR decomposition can be calculated using Gram-Schmidt process, or using Householder reflections. In practice, Householder reflections are more stable and efficient.

\subsubsection{Application to Least Squared}
In linear least squares problems, 
we aim to find a vector $\bm{x}$ that minimizes the Euclidean norm of the residual for an overdetermined system $\bm{Ax} \approx \bm{b}$, where $\bm{A} \in \mathbb{R}^{m \times n}$ and $m \ge n$.
The goal is to solve:$$\min_{\bm{x}} \|\bm{Ax} - \bm{b}\|_2$$Using the Full QR Decomposition, 
we substitute $\bm{A} = \bm{Q} \begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix}$:$$\|\bm{Ax} - \bm{b}\|_2 = \left\| \bm{Q} \begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix} \bm{x} - \bm{b} \right\|_2$$
Since multiplying by an orthogonal matrix $\bm{Q}$ preserves the Euclidean norm, 
we can left-multiply the entire expression by $\bm{Q}^T$:$$\|\bm{Ax} - \bm{b}\|_2 = \left\| \begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix} \bm{x} - \bm{Q}^T \bm{b} \right\|_2$$
If we partition $\bm{Q}^T \bm{b}$ into two components—$\bm{c}_1 \in \mathbb{R}^n$ and $\bm{c}_2 \in \mathbb{R}^{m-n}$:$$\left\| \begin{bmatrix} \bm{R}_1\bm{x} - \bm{c}_1 \\ -\bm{c}_2 \end{bmatrix} \right\|_2^2 = \|\bm{R}_1\bm{x} - \bm{c}_1\|_2^2 + \|\bm{c}_2\|_2^2$$

To minimize the total error, we must make the first term zero. 
The least squares solution is found by solving the square, upper-triangular system:$$\bm{R}_1 \bm{x} = \bm{c}_1$$
Since $\bm R_1$ is upper triangular, we can efficiently solve this system using back substitution, \textbf{that's how we solve linear equations manually in algebra class}!
The remaining term $\|\bm{c}_2\|_2$ represents the minimum residual norm (the "error" of the fit).


\subsubsection{About orthogonal matrix}

Let \(\bm{Q}\in\mathbb{R}^{n\times n}\) be an orthogonal matrix.  
Write \(\bm{Q}\) in terms of its column vectors:
\[
\bm{Q}=\big[\bm{q}_1\; \bm{q}_2\; \cdots\; \bm{q}_n\big], 
\qquad 
\bm{q}_i^{\top}\bm{q}_j=\delta_{ij}.
\]
Then
\[
\bm{Q}^{\top}=
\begin{bmatrix}
\bm{q}_1^{\top}\\
\bm{q}_2^{\top}\\
\vdots\\
\bm{q}_n^{\top}
\end{bmatrix}.
\] 

\paragraph{Computation of \(\bm{Q}^{\top}\bm{Q}\).}
\[
\bm{Q}^{\top}\bm{Q}
=
\begin{bmatrix}
\bm{q}_1^{\top}\\
\bm{q}_2^{\top}\\
\vdots\\
\bm{q}_n^{\top}
\end{bmatrix}
\big[\bm{q}_1\; \bm{q}_2\; \cdots\; \bm{q}_n\big]
=
\begin{bmatrix}
\bm{q}_1^{\top}\bm{q}_1 & \bm{q}_1^{\top}\bm{q}_2 & \cdots & \bm{q}_1^{\top}\bm{q}_n\\
\bm{q}_2^{\top}\bm{q}_1 & \bm{q}_2^{\top}\bm{q}_2 & \cdots & \bm{q}_2^{\top}\bm{q}_n\\
\vdots & \vdots & \ddots & \vdots\\
\bm{q}_n^{\top}\bm{q}_1 & \bm{q}_n^{\top}\bm{q}_2 & \cdots & \bm{q}_n^{\top}\bm{q}_n
\end{bmatrix}.
\]
Using \(\bm{q}_i^{\top}\bm{q}_j=\delta_{ij}\),
\[
\bm{Q}^{\top}\bm{Q}
=
\begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{bmatrix}
= \bm{I}.
\]

\paragraph{Computation of \(\bm{Q}\bm{Q}^{\top}\).}
\[
\bm{Q}\bm{Q}^{\top}
=
\big[\bm{q}_1\; \bm{q}_2\; \cdots\; \bm{q}_n\big]
\begin{bmatrix}
\bm{q}_1^{\top}\\
\bm{q}_2^{\top}\\
\vdots\\
\bm{q}_n^{\top}
\end{bmatrix}
=
\bm{q}_1\bm{q}_1^{\top}
+
\bm{q}_2\bm{q}_2^{\top}
+
\cdots
+
\bm{q}_n\bm{q}_n^{\top}
=
\sum_{k=1}^{n}\bm{q}_k\bm{q}_k^{\top}.
\]
For any \(\bm{x}\in\mathbb{R}^{n}\),
\[
(\bm{Q}\bm{Q}^{\top})\bm{x}
=
\sum_{k=1}^{n}\bm{q}_k(\bm{q}_k^{\top}\bm{x})
= \bm{x},
\]
since \(\{\bm{q}_1,\ldots,\bm{q}_n\}\) is an orthonormal basis of \(\mathbb{R}^{n}\).
Hence
\[
\bm{Q}\bm{Q}^{\top}=\bm{I}.
\]

Therefore, for an orthogonal matrix \(\bm{Q}\),
\[
\bm{Q}^{\top}\bm{Q}=\bm{Q}\bm{Q}^{\top}=\bm{I}.
\]

\subsubsection{SVD and orthogonal matrix}
There are two kinds of SVD: full SVD and reduced SVD. Let $\bm X \in \mathbb R ^{N\times p}$, and nomally $N > p$.
    \begin{itemize}
        \item \textbf{Full SVD} in standard linear algebra.
         $$\bm X = \bm U_{\text{full}} \bm D_{\text{full}} \bm V^T$$
         \begin{itemize}
            \item $\bm U_{\text{full}} \in \mathbb R^{N \times N}$ is orthogonal matrix, whose columns are eigenvectors of $\bm X \bm X^T$.
            \item $\bm D_{\text{full}} \in \mathbb R^{N \times p}$ is a rectangular diagonal matrix, and the last $N-p$ rows are all zero.
            \item $\bm V \in \mathbb R^{p \times p}$ is orthogonal matrix, whose columns are eigenvectors of $\bm X^T \bm X$, and are basis of row space of $\bm X$.
         \end{itemize}
        \item \textbf{Reduced SVD} in linear regression. We simply ignore the zero parts of $\bm U$ and $\bm D$.
        
        $\bm D$ and $\bm U$ can be seen as
        \[
        \bm{D}_{\text{full}} = \begin{bmatrix}
            d_1 & 0 & \dots \\
            0 & \ddots & \\
            \dots & & d_p \\
            \hline
            0 & 0 & 0 \\
            \vdots & \vdots & \vdots \\
            0 & 0 & 0
            \end{bmatrix}
            \begin{matrix}
            \left.\begin{matrix} \\ \\ \\ \end{matrix}\right\} p \text{ (Non-zero part)} \\
            \left.\begin{matrix} \\ \\ \\ \end{matrix}\right\} N-p \text{ (All zero part)}
        \end{matrix}
        \]
        \[\mathbf{U}_{\text{full}} = \begin{bmatrix}
            \mathbf{U}_1 & | & \mathbf{U}_2
            \end{bmatrix}\]
        Thus, 
        \[\begin{aligned}
        \mathbf{U}_{full} \cdot \mathbf{D}_{full} &= \begin{bmatrix} \mathbf{U}_1 & \mathbf{U}_2 \end{bmatrix} \cdot \begin{bmatrix} \mathbf{D}_p \\ \mathbf{0} \end{bmatrix} \\
        &= \mathbf{U}_1 \cdot \mathbf{D}_p + \mathbf{U}_2 \cdot \mathbf{0} \\
        &= \mathbf{U}_1 \cdot \mathbf{D}_p
        \end{aligned}\]
        That's the reduced SVD:
        $$\bm X = \bm U \bm D \bm V^T$$
        where $\bm U \in \mathbb R^{N \times p}$ with orthonormal columns, $\bm D \in \mathbb R^{p \times p}$ diagonal with positive entries, and $\bm V \in \mathbb R^{p \times p}$ orthogonal.



\end{itemize}


In regression, we usually use reduced SVD, and $\bm U \bm U^T = \bm H \ne \bm I$, but $\bm U^T \bm U = \bm I$.
For $\bm y \in \mathbb R^N$, $\bm U^T \bm y $ map $\bm y$ from $\mathbb R^N$ to $\mathbb R^p$, which is the coefficients in the basis of columns of $\bm U$.
And $\bm U \bm U^T \bm y$ project $\bm y$ onto the column space of $\bm X$.



\subsection{Multiple testing in forward selection}
ESL page 60:
\begin{quote}
    Other more traditional packages base the selection on F -statistics, adding “significant” terms, and dropping “non-significant” terms. These are out of fashion, since they do not take proper account of the multiple testing issues.
\end{quote}

Assume we have $p$ candidate features, and already selected $k$ features.
When considering adding a new feature, we are actually performing $p-k$ hypothesis tests (each test for one feature).
Even the rest $p-k$ features are all noise, with significance level $\alpha$, we still have a probability of $1-(1-\alpha)^{p-k}$ to incorrectly add at least one noise feature.

\subsection{Ridge regression}

Answer questions: why two forms are equivalent? Why not equivariant under scaling of the inputs? What is a good practice for it?
df of ridge? In the case of orthogonal inputs, why $\hat \beta^{\text{ridge}} = \hat \beta / (1+\lambda)$?

Ridge regression shrinks the regression coefficients by imposing a penalty on their size:
$$
\hat \beta^{\text{ridge}} = \arg\min_{\beta} \left\{ \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\}
$$
and is equivalent to 
\begin{align*}
    \hat \beta^{\text{ridge}} &= \arg\min_{\beta} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 \\
    &\text{subject to } \sum_{j=1}^p \beta_j^2 \le t
\end{align*}
And there is a one-to-one correspondence between $\lambda$ and $t$.

\subsubsection{How to use ridge regression}
Apparently, ridge regression is not equivariant under scaling of the inputs.
For example, using OLS, measuring $x$ in meters or in centimeters will not change the predictions, since the latter coefficients will just 100 times of the first.
However, in ridge regression, the penalty term $\lambda \sum_{j=1}^p \beta_j^2$ will be affected by the scale of $x_j$.
Which means, using centimeters instead of meters will make the penalty on $\beta_j$ 10000 times larger, leading to different solutions.
Thus, it's important to standardize the features (zero mean and unit variance) before applying ridge.

Usually, we calculate $\mu$ and $\sigma$ from training set, and use them to standardize both training and test sets.
Scaler can be regarded as part of the model, not data cleaning!
\subsubsection{Equivalence of two forms}
First, take a review of \textbf{KKT conditions}.

Condider a minimization problem with both equality and inequality constraints:
\begin{align*}
    \min_{\bm x} & f(\bm x) \\
    \text{subject to } & g_i(\bm x) \le 0,
    & i = 1, \ldots, m \\
    & h_j(\bm x) = 0,
    & j = 1, \ldots, l
\end{align*}
And the lagrangian function is:
$$
L(\bm x, \bm \lambda, \bm \mu) = f(\bm x) + \sum_{i=1}^m \lambda_i g_i(\bm x) + \sum_{j=1}^l \mu_j h_j(\bm x)
$$
If $\bm x^*$ is a local minimum, then there exist multipliers $\lambda_i^* \ge 0$ and $\mu_j^*$ such that the following conditions hold:
\begin{itemize}
    \item \textbf{Stationary} 
    $$\nabla_x L(\bm x^*, \bm \lambda^*, \bm \mu^*) = 0$$
    
    \item \textbf{Primal feasibility}
    $$g_i(\bm x^*) \le 0, \quad i = 1, \ldots, m$$
    $$h_j(\bm x^*) = 0, \quad j = 1, \ldots, l$$
    The gradient can be regarded as the force to push a particle, primal stationary means the force of $\partial f(\bm x^*)$ is balanced by a linear sum of forces from constraints.
    \item \textbf{Dual feasibility}
    $$\lambda_i^* \ge 0, \quad i = 1, \ldots, m$$
    All the $\partial g_i(\bm x^*)$ forces must be one-sided, pointing inwards into the feasible set for $\bm x$.
    \item \textbf{Complementary slackness}
    $$\lambda_i^* g_i(\bm x^*) = 0, \quad i = 1, \ldots, m$$
    The force only activated when the particle is on the boundary of feasible set.
\end{itemize}

In ridge regression, we have:
$$
\mathcal L = \| \bm Y - \bm X \beta \|_2^2 + \alpha (\| \beta \|_2^2 - t)\\
$$
According to stationary condition:
$$
\nabla_\beta \mathcal L = -2 \bm X^T (\bm Y - \bm X \beta) + 2 \alpha \beta = 0
$$
On the other hand, solving the unconstrained form is
$$
\nabla_\beta \left( \| \bm Y - \bm X \beta \|_2^2 + \lambda \| \beta \|_2^2 \right) = -2 \bm X^T (\bm Y - \bm X \beta) + 2 \lambda \beta = 0
$$
Thus, if we set $\lambda = \alpha$, the two forms are equivalent. 

One step further, according to complementary slackness:
$$
\alpha (\| \beta \|_2^2 - t) = 0
$$
\textbf{From unconstrained form, given $\lambda$}, we can solve $\beta$, denote $\beta(\lambda)$, and define $t(\lambda) = \| \beta(\lambda) \|_2^2.$
Then, $\beta(\lambda)$ is also the solution of constrained form with $t = t(\lambda)$ (apparently it's on the boundary, and the coefficient $\alpha = \lambda$).

\textbf{From the constrained form, given $t$}, we can also solve $\beta$, denote $\beta(t)$. 
\begin{itemize}
    \item If $\| \beta(t) \|_2^2 < t$, then according to complementary slackness, $\alpha = 0$, which means no penalty, $\lambda=0$, back to OLS.
    \item If $\| \beta(t) \|_2^2 = t$, the boundary is effective, correspond to some $\alpha > 0$, and $\lambda = \alpha$.
\end{itemize}

\subsubsection{Bayesian view}

Assume prior $\bm \beta \sim \mathcal N(0, \tau^2 \bm I)$, and likelihood $\bm Y \sim \mathcal N(\bm X \beta, \sigma^2 \bm I)$.
Then the posterior is:
\begin{align*}
    p(\beta | \bm Y) &\propto p(\bm Y | \beta) p(\beta) \\
    &\propto \exp \left( -\frac{1}{2 \sigma^2} \| \bm Y - \bm X \beta \|_2^2 \right) \exp \left( -\frac{1}{2 \tau^2} \| \beta \|_2^2 \right) \\
    &\propto \exp \left( -\frac{1}{2} \left( \frac{1}{\sigma^2} \| \bm Y - \bm X \beta \|_2^2 + \frac{1}{\tau^2} \| \beta \|_2^2 \right) \right)
\end{align*}
Maximizing the posterior is equivalent to minimizing the negative log-posterior. 
The object is equivalent to ridge regression with $\lambda = \sigma^2 / \tau^2$.

However, it seems that from bayesian view, we don't have a hard constrain on the size of $\beta$, just a prior that $\beta$ is likely to be small.
But the constrained form of ridge regression directly enforce $\|\beta\|^2_2 \leq t$. Is it contradictory?

No. Actually, the posterior distribution of $\beta$ still have infinite support, meaning that $\beta$ can still be large with small probability.
Our MAP estimate of $\beta$ is the mode of the posterior, it's just a \textbf{point estimate}.
This point estimator can still satisfy the hard constraint $\|\beta\|^2_2 \leq t$ for some $t$.


\subsubsection{SVD in ridge}

The solution of ridge regression is:
$$
\hat \beta^{\text{ridge}} = (\bm X^T \bm X + \lambda \bm I)^{-1} \bm X^T \bm Y
$$
When $\bm X^T \bm X$ is singular (not full rank), OLS estimator is not defined, and adding $\lambda \bm I$ ensures that $\bm X^T \bm X + \lambda \bm I$ is positive definite and invertible.
    
Take reduced SVD of $\bm X = \bm U \bm D \bm V^T$, $r = \text{Rank}(\bm X)$, where $\bm U \in \mathbb R^{N \times r}, \bm D \in \mathbb R^{r \times r}, \bm V \in \mathbb R^{p \times r}$.
Then,
\begin{align*}
    \bm X^T \bm X + \lambda \bm I &= \bm V \bm D^T \bm U^T \bm U \bm D \bm V^T + \lambda \bm I \\
    &= \bm V \bm D^T \bm D \bm V^T + \lambda \bm V \bm V^T \\
    &= \bm V (\bm D^2  + \lambda \bm I) \bm V^T
\end{align*}
must be full rank and invertible.

It's also easy to see that in the case of orthogonal inputs (i.e., $\bm X^T \bm X = \bm I$), the ridge estimates are just a scaled version of the least  squares estimates, that is
$$\hat \beta^{\text{ridge}} = \frac{1}{1+\lambda} \hat \beta$$

Further more,
\begin{align*}
    \bm X \beta^{\text{OLS}} &= \bm X (\bm X^T \bm X)^{-1} \bm X^T \bm Y \\
    &= \bm U \bm D \bm V^T (\bm V \bm D^T \bm U^T \bm U \bm D \bm V^T)^{-1} \bm V \bm D^T \bm U^T \bm Y \\
    &= \bm U \bm D \bm D^{-2} \bm D^T \bm U^T \bm Y \\
    &= \bm U \bm U^T \bm Y
\end{align*}
and
\begin{align*}
    \bm X \beta^{\text{ridge}} &= \bm X (\bm X^T \bm X + \lambda \bm I)^{-1} \bm X^T \bm Y \\
    &= \bm U \bm D \bm V^T (\bm V (\bm D^2 + \lambda \bm I) \bm V^T)^{-1} \bm V \bm D^T \bm U^T \bm Y \\
    &= \bm U \bm D (\bm D^2 + \lambda \bm I)^{-1} \bm D^T \bm U^T \bm Y \\
    &= \bm U \text{diag}\left( \frac{d_1^2}{d_1^2 + \lambda}, \frac{d_2^2}{d_2^2 + \lambda}, \ldots, \frac{d_r^2}{d_r^2 + \lambda} \right) \bm U^T \bm Y
\end{align*}

In both way, $\bm U^T \bm Y$ gets the coordinates of $\bm Y$ in the basis of columns of $\bm U$ (the principal components of $\bm X$),
and then send back to original space using $\bm U$.
But ridge regression shrink the coordinates.


\newpage
\begin{thebibliography}{9}
\bibitem{wiki_gauss}
Wikipedia contributors, "Gauss–Markov theorem," \textit{Wikipedia, The Free Encyclopedia}, \url{https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem} (accessed Dec 29, 2025).
\end{thebibliography}

\end{document}
