\documentclass[11pt, a4paper, scheme=plain]{ctexart} 
% 'scheme=plain' keeps headings in English style (Section 1) 
% while 'ctexart' ensures Chinese characters render correctly.

% --- Basic Packages ---
\usepackage{geometry}     % Page layout
\usepackage{graphicx}     % Images
\usepackage{float}        % Force image placement (H)
\usepackage{amsmath}      % Math formulas
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xcolor}       % Colors
\usepackage{listings}     % Code blocks
\usepackage{hyperref}     % Hyperlinks
\usepackage{enumitem}     % Better lists
\usepackage{booktabs} % 用于绘制更专业的表格横线
\usepackage{siunitx}  % 用于对齐小数点
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}
\let\bf\mathbf
% --- Layout Settings ---
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\setlength{\parindent}{0pt}   % No indentation for paragraphs (better for logs)
\setlength{\parskip}{0.8em}   % Space between paragraphs

% --- Code Block Style ---
\definecolor{bg}{rgb}{0.96,0.96,0.96}
\lstset{
    backgroundcolor=\color{bg},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    columns=fullflexible,
    keepspaces=true
}

% --- Custom Commands for Logging ---
% 1. Timestamp: Usage \timecheck{14:30}
\newcommand{\timecheck}[1]{\textbf{\textcolor{blue}{[#1]}}}

% 2. Inline Code: Usage \code{filename.py}
\newcommand{\code}[1]{\colorbox{bg}{\texttt{#1}}}

% --- Metadata ---
\title{\textbf{Element of Statistical Learning Note}}
\author{Zichong Wang}
\date{\today}

\begin{document}

\maketitle

% --- Section 1: Setup ---
\section{Chap 3: Linear Methods for Regression}

\subsection{Confidence region for \texorpdfstring{$\hat{\beta}$}{beta hat}}

We know that $\hat \beta \sim \mathcal N (\beta, \sigma^2 (\bm X^T\bm X)^{-1})$,
thus $\hat \beta - \beta \sim \mathcal N (0, \sigma^2 (\bm X^T\bm X)^{-1})$.

Since for $\bm z \sim \mathcal N (0, \bm \Sigma)$, we have $\bm z^T \bm \Sigma^{-1} \bm z \sim \chi^2_k$ , where $k = \text{rank}(\bm \Sigma)$,
we have 

$$(\hat \beta - \beta)^T (\sigma^2 (\bm X^T\bm X)^{-1})^{-1} (\hat \beta - \beta) \sim \chi^2_{p+1}$$

Thus, 

$$ \frac{1}{\sigma^2} (\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) \sim \chi^2_{p+1} $$

And have approx confidence region

$$(\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) \le \hat{\sigma}^2 \chi^2_{p+1, 1-\alpha} $$

Actually, $\hat \sigma^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2 = \frac{1}{N-p-1} \text{RSS}$, and $\text{RSS}/\sigma^2 \sim \chi^2_{N-p-1}$,

$$ \frac{(\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) / (p+1)}{\hat{\sigma}^2} \sim F_{p+1, N-p-1} $$

\subsection{What is Linear}

In the context of \textbf{linear model}, we are talking about linearity in parameters, meaning that the prediction $\hat{y}$ is a linear combination of the parameters $\beta_j$.
The $\bm X$ itself can be non-linear transformations of the original features, e.g., polynomial terms, interaction terms, etc.
$y = 1 / (\beta_0 + \beta_1 x)$ and $y = \beta_0 e^{\beta_1 x}$ are not linear models, since they're not linear in parameters.

In the context of \textbf{Linear estimators}, we are talking about the estimator $\hat \theta$ (e.g. $\hat \beta$) can be written as 
a linear combination of the observed response values $y_i$, i.e. $\hat \theta = \bm c^T \bm y$. The weight $\bm c$ depends only on $\bm X$, not on $\bm y$.
A linear estimator \textbf{can be} a prediction at a new point, or the estimated coefficients $\hat \beta$ themselves.

\subsection{Gauss-Markov Theorem}

Why assume only know $\bm X$, but not $\bm y$?

\begin{quote}
     Note that though $y_i$ as sample responses, are observable, 
     the following statements and arguments including assumptions, proofs and the others assume under the only condition of knowing $\bm X_{i,j}$ but not $y_i$. \hfill --- \cite{wiki_gauss}
\end{quote}

We have a \textit{challenger} linear estimator $\tilde{\beta} = \bm C \bm y$, where $\bm C = (\bm X^T \bm X)^{-1} \bm X^T + \bm D$, a modification of OLS estimator.
Ensure it's unbiased:

\begin{align*}
    \mathbb E(\tilde{\beta}) &= \mathbb{E}(\bm C \bm y) \\
    &= \bm C \mathbb E(\bm y) = ((\bm X^T \bm X)^{-1} \bm X^T + \bm D) \bm X \beta \\
    &= \beta + \bm D \bm X \beta\\
    &= \beta
\end{align*}

Meaning $\bm D \bm X = 0$.

Now, compute the variance:

\begin{align*}
    \text{Var}(\tilde{\beta}) &= \text{Var}(\bm C \bm y) \\
    &= \bm C \text{Var}(\bm y) \bm C^T \\
    &= \sigma^2 \bm C \bm C^T \\
    &= \sigma^2 ((\bm X^T \bm X)^{-1} \bm X^T + \bm D) ((\bm X^T \bm X)^{-1} \bm X^T + \bm D)^T \\
    &= \sigma^2 ((\bm X^T \bm X)^{-1} + \bm D \bm D^T)\\
    &= \text{Var}(\hat{\beta}) + \sigma^2 \bm D \bm D^T
    &\ge \text{Var}(\hat{\beta})
\end{align*}

\subsection{QR decomposition}


\begin{thebibliography}{9}
\bibitem{wiki_gauss}
Wikipedia contributors, "Gauss–Markov theorem," \textit{Wikipedia, The Free Encyclopedia}, \url{https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem} (accessed Dec 29, 2025).
\end{thebibliography}

\end{document}
