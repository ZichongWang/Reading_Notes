\documentclass[11pt, a4paper, scheme=plain]{ctexart} 
% 'scheme=plain' keeps headings in English style (Section 1) 
% while 'ctexart' ensures Chinese characters render correctly.

% --- Basic Packages ---
\usepackage{geometry}     % Page layout
\usepackage{graphicx}     % Images
\usepackage{float}        % Force image placement (H)
\usepackage{amsmath}      % Math formulas
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xcolor}       % Colors
\usepackage{listings}     % Code blocks
\usepackage{hyperref}     % Hyperlinks
\usepackage{enumitem}     % Better lists
\usepackage{booktabs} % 用于绘制更专业的表格横线
\usepackage{siunitx}  % 用于对齐小数点
\usepackage{multirow}
\usepackage{url}
\usepackage[style=numeric, backend=biber]{biblatex}
\addbibresource{references.bib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\let\bf\mathbf
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
% --- Layout Settings ---
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\setlength{\parindent}{0pt}   % No indentation for paragraphs (better for logs)
\setlength{\parskip}{0.8em}   % Space between paragraphs

% --- Code Block Style ---
\definecolor{bg}{rgb}{0.96,0.96,0.96}
\lstset{
    backgroundcolor=\color{bg},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    columns=fullflexible,
    keepspaces=true
}

% --- Custom Commands for Logging ---
% 1. Timestamp: Usage \timecheck{14:30}
\newcommand{\timecheck}[1]{\textbf{\textcolor{blue}{[#1]}}}

% 2. Inline Code: Usage \code{filename.py}
\newcommand{\code}[1]{\colorbox{bg}{\texttt{#1}}}

% --- Metadata ---
\title{\textbf{Element of Statistical Learning Note}}
\author{Zichong Wang}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\clearpage

\section{Intro}

While reading ESL, I felt the need to write things down—to record what I did not understand 
and how I eventually worked through it.
This note grew out of that process.
It can be regarded as a personal supplymentary material of ESL.
It covers concepts from analysis, linear algebra, optimization, 
and computation, with a primary emphasis on statistics.

\newpage
\section{Chap 3: Linear Methods for Regression}

\subsection{Confidence region for \texorpdfstring{$\hat{\beta}$}{beta hat}}

We know that $\hat \beta \sim \mathcal N (\beta, \sigma^2 (\bm X^T\bm X)^{-1})$,
thus $\hat \beta - \beta \sim \mathcal N (0, \sigma^2 (\bm X^T\bm X)^{-1})$.

Since for $\bm z \sim \mathcal N (0, \bm \Sigma)$, we have $\bm z^T \bm \Sigma^{-1} \bm z \sim \chi^2_k$ , where $k = \text{rank}(\bm \Sigma)$,
we have 

$$(\hat \beta - \beta)^T (\sigma^2 (\bm X^T\bm X)^{-1})^{-1} (\hat \beta - \beta) \sim \chi^2_{p+1}$$

Thus, 

$$ \frac{1}{\sigma^2} (\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) \sim \chi^2_{p+1} $$

And have approx confidence region

$$(\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) \le \hat{\sigma}^2 \chi^2_{p+1, 1-\alpha} $$

Actually, $\hat \sigma^2 = \frac{1}{N-p-1} \sum_{i=1}^N (y_i - \hat{y}_i)^2 = \frac{1}{N-p-1} \text{RSS}$, and $\text{RSS}/\sigma^2 \sim \chi^2_{N-p-1}$,

$$ \frac{(\hat{\beta} - \beta)^T \bm{X}^T \bm{X} (\hat{\beta} - \beta) / (p+1)}{\hat{\sigma}^2} \sim F_{p+1, N-p-1} $$

\subsection{What is Linear}

In the context of \textbf{linear model}, we are talking about linearity in parameters, meaning that the prediction $\hat{y}$ is a linear combination of the parameters $\beta_j$.
The $\bm X$ itself can be non-linear transformations of the original features, e.g., polynomial terms, interaction terms, etc.
$y = 1 / (\beta_0 + \beta_1 x)$ and $y = \beta_0 e^{\beta_1 x}$ are not linear models, since they're not linear in parameters.

In the context of \textbf{Linear estimators}, we are talking about the estimator $\hat \theta$ (e.g. $\hat \beta$) can be written as 
a linear combination of the observed response values $y_i$, i.e. $\hat \theta = \bm c^T \bm y$. The weight $\bm c$ depends only on $\bm X$, not on $\bm y$.
A linear estimator \textbf{can be} a prediction at a new point, or the estimated coefficients $\hat \beta$ themselves.

\subsection{Gauss-Markov Theorem}

Why assume only know $\bm X$, but not $\bm y$?

\begin{quote}
     Note that though $y_i$ as sample responses, are observable, 
     the following statements and arguments including assumptions, proofs and the others assume under the only condition of knowing $\bm X_{i,j}$ but not $y_i$. \hfill --- \cite{WikiGaussMarkov}
\end{quote}

We have a \textit{challenger} linear estimator $\tilde{\beta} = \bm C \bm y$, where $\bm C = (\bm X^T \bm X)^{-1} \bm X^T + \bm D$, a modification of OLS estimator.
Ensure it's unbiased:
\begin{align*}
    \mathbb E(\tilde{\beta}) &= \mathbb{E}(\bm C \bm y) \\
    &= \bm C \mathbb E(\bm y) = ((\bm X^T \bm X)^{-1} \bm X^T + \bm D) \bm X \beta \\
    &= \beta + \bm D \bm X \beta\\
    &= \beta
\end{align*}
Meaning $\bm D \bm X = 0$.

Now, compute the variance:
\begin{align*}
    \text{Var}(\tilde{\beta}) &= \text{Var}(\bm C \bm y) \\
    &= \bm C \text{Var}(\bm y) \bm C^T \\
    &= \sigma^2 \bm C \bm C^T \\
    &= \sigma^2 ((\bm X^T \bm X)^{-1} \bm X^T + \bm D) ((\bm X^T \bm X)^{-1} \bm X^T + \bm D)^T \\
    &= \sigma^2 ((\bm X^T \bm X)^{-1} + \bm D \bm D^T)\\
    &= \text{Var}(\hat{\beta}) + \sigma^2 \bm D \bm D^T\\
    &\ge \text{Var}(\hat{\beta})
\end{align*}

\subsection{QR decomposition}

Any real squared matrix $\bm A$ can be decomposed as $\bm A = \bm Q \bm R$, where $\bm Q$ is orthogonal matrix ($\bm Q^T \bm Q = \bm I$), and $\bm R$ is upper triangular matrix.

Any rectangular matrix $\bm A \in \mathbb R^{m \times n}$ ($m \ge n$), we can decompose it as $\bm A = \bm Q \bm R$, where $\bm Q$ is $m \times m$ orthogonal matrix, and $\bm R$ is $m \times n$ upper triangular matrix (the last $m-n$ rows are all zero).
It can be regarded as $\bm A = \bm Q \bm R=[\bm Q_1\quad \bm Q_2]\begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix} = \bm{Q}_1\bm{R}_1,$
$\text{where } \bm{Q}_1 \in \mathbb{R}^{m \times n}, \bm{Q}_2 \in \mathbb{R}^{m \times (m-n)}, \bm{R}_1 \in \mathbb{R}^{n \times n} \text{ which is upper triangular}$.

If $\bm A$ have $k$ linearly independent columns, then first $k$ columns of $\bm Q$ form an orthonormal basis of the column space of $\bm A$.
The fact that any column $k$ of $\bm A$ only depends on the first $k$ columns of $\bm Q$ corresponds to the triangular form of $\bm R$.


QR decomposition can be calculated using Gram-Schmidt process, or using Householder reflections. In practice, Householder reflections are more stable and efficient.

\subsubsection{Application to Least Squared}
In linear least squares problems, 
we aim to find a vector $\bm{x}$ that minimizes the Euclidean norm of the residual for an overdetermined system $\bm{Ax} \approx \bm{b}$, where $\bm{A} \in \mathbb{R}^{m \times n}$ and $m \ge n$.
The goal is to solve:$$\min_{\bm{x}} \|\bm{Ax} - \bm{b}\|_2$$Using the Full QR Decomposition, 
we substitute $\bm{A} = \bm{Q} \begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix}$:$$\|\bm{Ax} - \bm{b}\|_2 = \left\| \bm{Q} \begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix} \bm{x} - \bm{b} \right\|_2$$
Since multiplying by an orthogonal matrix $\bm{Q}$ preserves the Euclidean norm, 
we can left-multiply the entire expression by $\bm{Q}^T$:$$\|\bm{Ax} - \bm{b}\|_2 = \left\| \begin{bmatrix} \bm{R}_1 \\ \bm{0} \end{bmatrix} \bm{x} - \bm{Q}^T \bm{b} \right\|_2$$
If we partition $\bm{Q}^T \bm{b}$ into two components—$\bm{c}_1 \in \mathbb{R}^n$ and $\bm{c}_2 \in \mathbb{R}^{m-n}$:$$\left\| \begin{bmatrix} \bm{R}_1\bm{x} - \bm{c}_1 \\ -\bm{c}_2 \end{bmatrix} \right\|_2^2 = \|\bm{R}_1\bm{x} - \bm{c}_1\|_2^2 + \|\bm{c}_2\|_2^2$$

To minimize the total error, we must make the first term zero. 
The least squares solution is found by solving the square, upper-triangular system:$$\bm{R}_1 \bm{x} = \bm{c}_1$$
Since $\bm R_1$ is upper triangular, we can efficiently solve this system using back substitution, \textbf{that's how we solve linear equations manually in algebra class}!
The remaining term $\|\bm{c}_2\|_2$ represents the minimum residual norm (the "error" of the fit).


\subsubsection{About orthogonal matrix}

Let \(\bm{Q}\in\mathbb{R}^{n\times n}\) be an orthogonal matrix.  
Write \(\bm{Q}\) in terms of its column vectors:
\[
\bm{Q}=\big[\bm{q}_1\; \bm{q}_2\; \cdots\; \bm{q}_n\big], 
\qquad 
\bm{q}_i^{\top}\bm{q}_j=\delta_{ij}.
\]
Then
\[
\bm{Q}^{\top}=
\begin{bmatrix}
\bm{q}_1^{\top}\\
\bm{q}_2^{\top}\\
\vdots\\
\bm{q}_n^{\top}
\end{bmatrix}.
\] 

\paragraph{Computation of \(\bm{Q}^{\top}\bm{Q}\).}
\[
\bm{Q}^{\top}\bm{Q}
=
\begin{bmatrix}
\bm{q}_1^{\top}\\
\bm{q}_2^{\top}\\
\vdots\\
\bm{q}_n^{\top}
\end{bmatrix}
\big[\bm{q}_1\; \bm{q}_2\; \cdots\; \bm{q}_n\big]
=
\begin{bmatrix}
\bm{q}_1^{\top}\bm{q}_1 & \bm{q}_1^{\top}\bm{q}_2 & \cdots & \bm{q}_1^{\top}\bm{q}_n\\
\bm{q}_2^{\top}\bm{q}_1 & \bm{q}_2^{\top}\bm{q}_2 & \cdots & \bm{q}_2^{\top}\bm{q}_n\\
\vdots & \vdots & \ddots & \vdots\\
\bm{q}_n^{\top}\bm{q}_1 & \bm{q}_n^{\top}\bm{q}_2 & \cdots & \bm{q}_n^{\top}\bm{q}_n
\end{bmatrix}.
\]
Using \(\bm{q}_i^{\top}\bm{q}_j=\delta_{ij}\),
\[
\bm{Q}^{\top}\bm{Q}
=
\begin{bmatrix}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{bmatrix}
= \bm{I}.
\]

\paragraph{Computation of \(\bm{Q}\bm{Q}^{\top}\).}
\[
\bm{Q}\bm{Q}^{\top}
=
\big[\bm{q}_1\; \bm{q}_2\; \cdots\; \bm{q}_n\big]
\begin{bmatrix}
\bm{q}_1^{\top}\\
\bm{q}_2^{\top}\\
\vdots\\
\bm{q}_n^{\top}
\end{bmatrix}
=
\bm{q}_1\bm{q}_1^{\top}
+
\bm{q}_2\bm{q}_2^{\top}
+
\cdots
+
\bm{q}_n\bm{q}_n^{\top}
=
\sum_{k=1}^{n}\bm{q}_k\bm{q}_k^{\top}.
\]
For any \(\bm{x}\in\mathbb{R}^{n}\),
\[
(\bm{Q}\bm{Q}^{\top})\bm{x}
=
\sum_{k=1}^{n}\bm{q}_k(\bm{q}_k^{\top}\bm{x})
= \bm{x},
\]
since \(\{\bm{q}_1,\ldots,\bm{q}_n\}\) is an orthonormal basis of \(\mathbb{R}^{n}\).
Hence
\[
\bm{Q}\bm{Q}^{\top}=\bm{I}.
\]

Therefore, for an orthogonal matrix \(\bm{Q}\),
\[
\bm{Q}^{\top}\bm{Q}=\bm{Q}\bm{Q}^{\top}=\bm{I}.
\]

\subsubsection{SVD and orthogonal matrix}
There are two kinds of SVD: full SVD and reduced SVD. Let $\bm X \in \mathbb R ^{N\times p}$, and nomally $N > p$.
    \begin{itemize}
        \item \textbf{Full SVD} in standard linear algebra.
         $$\bm X = \bm U_{\text{full}} \bm D_{\text{full}} \bm V^T$$
         \begin{itemize}
            \item $\bm U_{\text{full}} \in \mathbb R^{N \times N}$ is orthogonal matrix, whose columns are eigenvectors of $\bm X \bm X^T$.
            \item $\bm D_{\text{full}} \in \mathbb R^{N \times p}$ is a rectangular diagonal matrix, and the last $N-p$ rows are all zero.
            \item $\bm V \in \mathbb R^{p \times p}$ is orthogonal matrix, whose columns are eigenvectors of $\bm X^T \bm X$, and are basis of row space of $\bm X$.
         \end{itemize}
        \item \textbf{Reduced SVD} in linear regression. We simply ignore the zero parts of $\bm U$ and $\bm D$.
        
        $\bm D$ and $\bm U$ can be seen as
        \[
        \bm{D}_{\text{full}} = \begin{bmatrix}
            d_1 & 0 & \dots \\
            0 & \ddots & \\
            \dots & & d_p \\
            \hline
            0 & 0 & 0 \\
            \vdots & \vdots & \vdots \\
            0 & 0 & 0
            \end{bmatrix}
            \begin{matrix}
            \left.\begin{matrix} \\ \\ \\ \end{matrix}\right\} p \text{ (Non-zero part)} \\
            \left.\begin{matrix} \\ \\ \\ \end{matrix}\right\} N-p \text{ (All zero part)}
        \end{matrix}
        \]
        \[\mathbf{U}_{\text{full}} = \begin{bmatrix}
            \mathbf{U}_1 & | & \mathbf{U}_2
            \end{bmatrix}\]
        Thus, 
        \[\begin{aligned}
        \mathbf{U}_{full} \cdot \mathbf{D}_{full} &= \begin{bmatrix} \mathbf{U}_1 & \mathbf{U}_2 \end{bmatrix} \cdot \begin{bmatrix} \mathbf{D}_p \\ \mathbf{0} \end{bmatrix} \\
        &= \mathbf{U}_1 \cdot \mathbf{D}_p + \mathbf{U}_2 \cdot \mathbf{0} \\
        &= \mathbf{U}_1 \cdot \mathbf{D}_p
        \end{aligned}\]
        That's the reduced SVD:
        $$\bm X = \bm U \bm D \bm V^T$$
        where $\bm U \in \mathbb R^{N \times p}$ with orthonormal columns, $\bm D \in \mathbb R^{p \times p}$ diagonal with positive entries, and $\bm V \in \mathbb R^{p \times p}$ orthogonal.



\end{itemize}


In regression, we usually use reduced SVD, and $\bm U \bm U^T = \bm H \ne \bm I$, but $\bm U^T \bm U = \bm I$.
For $\bm y \in \mathbb R^N$, $\bm U^T \bm y $ map $\bm y$ from $\mathbb R^N$ to $\mathbb R^p$, which is the coefficients in the basis of columns of $\bm U$.
And $\bm U \bm U^T \bm y$ project $\bm y$ onto the column space of $\bm X$.



\subsection{Multiple testing in forward selection}
ESL page 60:
\begin{quote}
    Other more traditional packages base the selection on F -statistics, adding “significant” terms, and dropping “non-significant” terms. These are out of fashion, since they do not take proper account of the multiple testing issues.
\end{quote}

Assume we have $p$ candidate features, and already selected $k$ features.
When considering adding a new feature, we are actually performing $p-k$ hypothesis tests (each test for one feature).
Even the rest $p-k$ features are all noise, with significance level $\alpha$, we still have a probability of $1-(1-\alpha)^{p-k}$ to incorrectly add at least one noise feature.

\subsection{Ridge regression}

Answer questions: why two forms are equivalent? Why not equivariant under scaling of the inputs? What is a good practice for it?
df of ridge? In the case of orthogonal inputs, why $\hat \beta^{\text{ridge}} = \hat \beta / (1+\lambda)$?

Ridge regression shrinks the regression coefficients by imposing a penalty on their size:
$$
\hat \beta^{\text{ridge}} = \arg\min_{\beta} \left\{ \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\}
$$
and is equivalent to 
\begin{align*}
    \hat \beta^{\text{ridge}} &= \arg\min_{\beta} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j)^2 \\
    &\text{subject to } \sum_{j=1}^p \beta_j^2 \le t
\end{align*}
And there is a one-to-one correspondence between $\lambda$ and $t$.

\subsubsection{How to use ridge regression}
Apparently, ridge regression is not equivariant under scaling of the inputs.
For example, using OLS, measuring $x$ in meters or in centimeters will not change the predictions, since the latter coefficients will just 100 times of the first.
However, in ridge regression, the penalty term $\lambda \sum_{j=1}^p \beta_j^2$ will be affected by the scale of $x_j$.
Which means, using centimeters instead of meters will make the penalty on $\beta_j$ 10000 times larger, leading to different solutions.
Thus, it's important to standardize the features (zero mean and unit variance) before applying ridge.

Usually, we calculate $\mu$ and $\sigma$ from training set, and use them to standardize both training and test sets.
Scaler can be regarded as part of the model, not data cleaning!
\subsubsection{Equivalence of two forms}
First, take a review of \textbf{KKT conditions}.

Condider a minimization problem with both equality and inequality constraints:
\begin{align*}
    \min_{\bm x} & f(\bm x) \\
    \text{subject to } & g_i(\bm x) \le 0,
    & i = 1, \ldots, m \\
    & h_j(\bm x) = 0,
    & j = 1, \ldots, l
\end{align*}
And the lagrangian function is:
$$
L(\bm x, \bm \lambda, \bm \mu) = f(\bm x) + \sum_{i=1}^m \lambda_i g_i(\bm x) + \sum_{j=1}^l \mu_j h_j(\bm x)
$$
If $\bm x^*$ is a local minimum, then there exist multipliers $\lambda_i^* \ge 0$ and $\mu_j^*$ such that the following conditions hold:
\begin{itemize}
    \item \textbf{Stationary} 
    $$\nabla_x L(\bm x^*, \bm \lambda^*, \bm \mu^*) = 0$$
    
    \item \textbf{Primal feasibility}
    $$g_i(\bm x^*) \le 0, \quad i = 1, \ldots, m$$
    $$h_j(\bm x^*) = 0, \quad j = 1, \ldots, l$$
    The gradient can be regarded as the force to push a particle, primal stationary means the force of $\partial f(\bm x^*)$ is balanced by a linear sum of forces from constraints.
    \item \textbf{Dual feasibility}
    $$\lambda_i^* \ge 0, \quad i = 1, \ldots, m$$
    All the $\partial g_i(\bm x^*)$ forces must be one-sided, pointing inwards into the feasible set for $\bm x$.
    \item \textbf{Complementary slackness}
    $$\lambda_i^* g_i(\bm x^*) = 0, \quad i = 1, \ldots, m$$
    The force only activated when the particle is on the boundary of feasible set.
\end{itemize}

In ridge regression, we have:
$$
\mathcal L = \| \bm Y - \bm X \beta \|_2^2 + \alpha (\| \beta \|_2^2 - t)\\
$$
According to stationary condition:
$$
\nabla_\beta \mathcal L = -2 \bm X^T (\bm Y - \bm X \beta) + 2 \alpha \beta = 0
$$
On the other hand, solving the unconstrained form is
$$
\nabla_\beta \left( \| \bm Y - \bm X \beta \|_2^2 + \lambda \| \beta \|_2^2 \right) = -2 \bm X^T (\bm Y - \bm X \beta) + 2 \lambda \beta = 0
$$
Thus, if we set $\lambda = \alpha$, the two forms are equivalent. 

One step further, according to complementary slackness:
$$
\alpha (\| \beta \|_2^2 - t) = 0
$$
\textbf{From unconstrained form, given $\lambda$}, we can solve $\beta$, denote $\beta(\lambda)$, and define $t(\lambda) = \| \beta(\lambda) \|_2^2.$
Then, $\beta(\lambda)$ is also the solution of constrained form with $t = t(\lambda)$ (apparently it's on the boundary, and the coefficient $\alpha = \lambda$).

\textbf{From the constrained form, given $t$}, we can also solve $\beta$, denote $\beta(t)$. 
\begin{itemize}
    \item If $\| \beta(t) \|_2^2 < t$, then according to complementary slackness, $\alpha = 0$, which means no penalty, $\lambda=0$, back to OLS.
    \item If $\| \beta(t) \|_2^2 = t$, the boundary is effective, correspond to some $\alpha > 0$, and $\lambda = \alpha$.
\end{itemize}

\subsubsection{Bayesian view}

Assume prior $\bm \beta \sim \mathcal N(0, \tau^2 \bm I)$, and likelihood $\bm Y \sim \mathcal N(\bm X \beta, \sigma^2 \bm I)$.
Then the posterior is:
\begin{align*}
    p(\beta | \bm Y) &\propto p(\bm Y | \beta) p(\beta) \\
    &\propto \exp \left( -\frac{1}{2 \sigma^2} \| \bm Y - \bm X \beta \|_2^2 \right) \exp \left( -\frac{1}{2 \tau^2} \| \beta \|_2^2 \right) \\
    &\propto \exp \left( -\frac{1}{2} \left( \frac{1}{\sigma^2} \| \bm Y - \bm X \beta \|_2^2 + \frac{1}{\tau^2} \| \beta \|_2^2 \right) \right)
\end{align*}
Maximizing the posterior is equivalent to minimizing the negative log-posterior. 
The object is equivalent to ridge regression with $\lambda = \sigma^2 / \tau^2$.

However, it seems that from bayesian view, we don't have a hard constrain on the size of $\beta$, just a prior that $\beta$ is likely to be small.
But the constrained form of ridge regression directly enforce $\|\beta\|^2_2 \leq t$. Is it contradictory?

No. Actually, the posterior distribution of $\beta$ still have infinite support, meaning that $\beta$ can still be large with small probability.
Our MAP estimate of $\beta$ is the \textbf{mode} of the posterior, it's just a \textbf{point estimate}.
This point estimator can still satisfy the hard constraint $\|\beta\|^2_2 \leq t$ for some $t$.

In ridge regression, since the posterior is symmetric gaussian distribution, the mean and mode are same.
However, in lasso and other priors, we usually use \textbf{mode}, \textbf{not mean} in common Bayes estimate.


\subsubsection{SVD in ridge}

The solution of ridge regression is:
$$
\hat \beta^{\text{ridge}} = (\bm X^T \bm X + \lambda \bm I)^{-1} \bm X^T \bm Y
$$
When $\bm X^T \bm X$ is singular (not full rank), OLS estimator is not defined, and adding $\lambda \bm I$ ensures that $\bm X^T \bm X + \lambda \bm I$ is positive definite and invertible.
    
Take reduced SVD of $\bm X = \bm U \bm D \bm V^T$, $r = \text{Rank}(\bm X)$, where $\bm U \in \mathbb R^{N \times r}, \bm D \in \mathbb R^{r \times r}, \bm V \in \mathbb R^{p \times r}$.
Then,
\begin{align*}
    \bm X^T \bm X + \lambda \bm I &= \bm V \bm D^T \bm U^T \bm U \bm D \bm V^T + \lambda \bm I \\
    &= \bm V \bm D^T \bm D \bm V^T + \lambda \bm V \bm V^T \\
    &= \bm V (\bm D^2  + \lambda \bm I) \bm V^T
\end{align*}
must be full rank and invertible.

It's also easy to see that in the case of orthogonal inputs (i.e., $\bm X^T \bm X = \bm I$), the ridge estimates are just a scaled version of the least  squares estimates, that is
$$\hat \beta^{\text{ridge}} = \frac{1}{1+\lambda} \hat \beta$$

Further more,
\begin{align*}
    \bm X \beta^{\text{OLS}} &= \bm X (\bm X^T \bm X)^{-1} \bm X^T \bm Y \\
    &= \bm U \bm D \bm V^T (\bm V \bm D^T \bm U^T \bm U \bm D \bm V^T)^{-1} \bm V \bm D^T \bm U^T \bm Y \\
    &= \bm U \bm D \bm D^{-2} \bm D^T \bm U^T \bm Y \\
    &= \bm U \bm U^T \bm Y
\end{align*}
and
\begin{align*}
    \bm X \beta^{\text{ridge}} &= \bm X (\bm X^T \bm X + \lambda \bm I)^{-1} \bm X^T \bm Y \\
    &= \bm U \bm D \bm V^T (\bm V (\bm D^2 + \lambda \bm I) \bm V^T)^{-1} \bm V \bm D^T \bm U^T \bm Y \\
    &= \bm U \bm D (\bm D^2 + \lambda \bm I)^{-1} \bm D^T \bm U^T \bm Y \\
    &= \bm U \text{diag}\left( \frac{d_1^2}{d_1^2 + \lambda}, \frac{d_2^2}{d_2^2 + \lambda}, \ldots, \frac{d_r^2}{d_r^2 + \lambda} \right) \bm U^T \bm Y
\end{align*}

In both way, $\bm U^T \bm Y$ gets the coordinates of $\bm Y$ in the basis of columns of $\bm U$ (the principal components of $\bm X$),
and then send back to original space using $\bm U$,
but ridge regression shrinks the coordinates.
It's easy to see the directions with larger variance have lower effect, and those with small variance shrink most.

Similar to OLS, the degree of freedom of ridge is defined as 
$$
\text{df}(\lambda) = \text{tr}(\bm H(\lambda)) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}
$$

But actually it's because the definition of df is
$$
\text{df}(\bm{\hat y}) = \frac{1}{\sigma^2}\text{Cov}(\hat y_i, y_i)
$$
indicating how much the prediction $\hat y_i$ change if data $y_i$ have a tiny change.
If the change is big, then model is 'reciting' the data point, model is complex, taking lots of df.
If the change is small, meaning model is smooth, little affected by single data point. Model is simple, df is low.

For those prediction is a linear transformation of $\bm y$ (OLS, ridge), the df is the trace of hat matrix.

\subsubsection{Revisit PCA}
For a matrix $\bm X \in \mathbb R^{n\times p}$, the mean is 
$$
\mu := \frac{1}{n}\bm X^T \bm 1 \in \mathbb R^{p}
$$
The centered matrix is 
$$
\bm X_c := \bm X - \bm 1  \mu^T
$$
centering is to put the center of data cloud in origin, otherwise PCs are not purely about variance, but also about the shift from origin.

The SVD of $\bm X_c$ is $\bm X_c = \bm U \bm D \bm V^T$, and since it's centered, the covariance is
$$
\bm S = \frac{1}{n-1}\bm X_c^T \bm X_c = \frac{1}{n-1}\bm V \bm D^2 \bm V^T
$$
apparently, each column of $\bm V$ is the eigenvector of $\bm S$: $v_k = \bm V_{:,k} \in \mathbb R^p$, 
and project origin $p$ dim feature into one dim.

Thus, to get a $k$ dim representation, we need to take first $k$ column of $\bm V$, which is an encoder:
$$\bm Z_k = \bm X_c \bm V_k = \bm U_k \bm \Sigma_k$$
The first principal component direction $z_1 = \bm X_c v_1$ 
has \textbf{largest} sample variance among all normalized linear combinations of the columns of $\bm X_c$,
the variance is 
\begin{align*}
    \text{Var}(z_1) &= \frac{1}{n-1}z_1^T z_1 = \frac{1}{n-1}(\bm X_c v_1)^T(\bm X_c v_1)\\
    &= \frac{1}{n-1}v_1^T \bm X_c^T \bm X_c v_1 \\
    &= v_1^T \bm S v_1\\
    &= v_1^T \lambda_1 v_1\\
    &= \lambda_1 = \frac{\sigma_1^2}{n-1}
\end{align*}
So the variance of the first PC scores is exactly the top eigenvalue of the covariance matrix.

And if we need to reconstruct the origin matrix, just decode in the same way:
$$
\bm {\hat X_c} = \bm Z_k \bm V_k^T, \quad \bm{\hat X} = \bm {\hat X_c} + \bm 1  \mu^T
$$
for a single sample:
$$\hat x = \bm V_k z = \bm V_k (\bm V_k^T z)$$
Thus, $\bm V$ is just orthogonal basis of the feature space. It maps centered data in to its space, and get the coordinates.

From an auto encoder's view, it is
$$\min_{\bm{W} \in \mathbb{R}^{p \times k}} \|\bm{X}_c - \bm{X}_c \bm{W} \bm{W}^\top \|_F^2 \quad \text{s.t.} \quad \bm{W}^\top \bm{W} = \bm{I}$$

It seems we are talking about $\bm V$ all the time. What about $\bm U$? Well, he is the basis of the sample space. 
Since we don't care about sample space that much, we just don't talk about it that much.


\subsection{LASSO}\label{LASSO}

\subsubsection{Subgradient and subdifferential}

For a convex function $f: \mathbb R \to \mathbb R$, a number $g$ is a subgradient of $f$ at $x_0$ if for all $x$,
$$f(x) \geq f(x_0) + g\cdot (x-x_0)$$ 
So the line with slope $g$ touching $(x_0, f(x_0))$ lies below the whole function.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/subgradient.jpeg}
    \caption{Illustration of subgradient.}
\end{figure}

The set of all such slopes is the subdifferential:
$$\partial f(x_0) = \{g: f(x) \geq f(x_0) + g\cdot (x-x_0), \forall x \}$$
Thus, for $f(x) = |x|$,
$$
\partial |x| =
\begin{cases}
\{ 1 \} & \text{if } x > 0 \\
\{ -1 \} & \text{if } x < 0 \\
[-1, 1] & \text{if } x = 0
\end{cases}
$$
When $x=0$, the subdifferential is a interval!

Common way to compute Lasso is ISAT in \ref{ISTA}.

\subsubsection{Why shrink to zero}
Consider LASSO objective function
$$\min_{\beta}\|\bm Y - \bm X \bm \beta\|^2_2 + \lambda \|\bm \beta\|_1$$
the solution $\hat{ \bm\beta}$ satisfies
$$\bm 0 \in \partial J(\hat \beta)\in \mathbb R^p$$
take one feature, and define $\bm r = \bm Y - \bm X \bm{\hat{\beta}}$
$$0 \in -X_j^T \bm r + \lambda \partial \hat \beta_j$$
which means, if $X_j^T \bm r \in [-\lambda, \lambda]$, $\hat \beta_j$ will be shrink to zero.

What does a low value of $\mathbf{x}_j^T \mathbf{r}$ actually indicate? 
It signifies that the feature has a low correlation with the current residual. 
In other words, this feature is not useful for reducing the prediction error, 
even if it contains 'new' or unique information.

We must distinguish this from \textbf{feature-to-feature correlation }(multicollinearity):

\begin{itemize}
    \item \textbf{Redundancy (High Feature Correlation):} 
    Consider predicting weight using height. 
    Height in 'cm' and 'inches' are perfectly correlated. 
    Lasso chooses only one because adding the second provides no \textit{additional} help in reducing the residual.
    \item \textbf{Irrelevance (Low Residual Correlation):}
    Consider the feature 'eating carrots'. This is uncorrelated with height (it brings independent information), 
    but it has no relationship with weight. 
    Since it cannot lower the residual ($\bm{x}_{carrots}^T \bm{r} \approx 0$), 
    Lasso abandons it.
    
\end{itemize}

\subsubsection{Soft thresholding}
When design matrix $\bm X$ is orthogonal, i.e. $\bm X^T \bm X = \bm I$, 
the OLS solution become $\bm{\hat \beta^{\text{OLS}}}=\bm X^T \bm Y$, and LASSO becomes
\begin{align*}
    \bm{\hat \beta^{\text{Lasso}}} &= \argmin_\beta \|\bm Y - \bm X \bm \beta \|^2_2 + \lambda \|\bm \beta\|_1 \\
    &= \argmin_\beta \|\bm X^T \bm Y - \bm \beta \|^2_2 + \lambda \|\bm \beta\|_1 \\
    &= \argmin_\beta \|\bm{\hat \beta^{\text{OLS}}} - \bm \beta \|^2_2 + \lambda \|\bm \beta\|_1 \\
\end{align*}
Let the gradient to zero,
\begin{align*}
    \frac{\partial}{\partial \bm \beta} &= \bm \beta - \bm{\hat \beta^{\text{OLS}}} + \lambda \cdot \text{sign}(\bm \beta) = 0 \\\\
    \Rightarrow \quad \bm{\hat \beta^{\text{Lasso}}} &= \bm{\hat \beta^{\text{OLS}}} - \lambda \cdot \text{sign}(\bm{\hat \beta^{\text{Lasso}}})\\
    &= \text{sign}(\bm{\hat \beta^{\text{OLS}}}) \max\{0, |\bm{\hat \beta^{\text{OLS}}}| - \lambda\} \\
    &= S_\lambda(\bm{\hat \beta^{\text{OLS}}})
\end{align*}
Here, $S_\lambda$ is the soft-thresholding operator.






\newpage
\section{Optimization methods}
I read \cite{TibshiraniConvexOpt} for convex optimization methods.

\subsection{Gradient descent}
Assume $f$ is convex and differentiable, with $\text{dom}(f)=\mathbb R^n$,
and additionally that $\nabla f$ is Lipschitz continuous with constant $L > 0$,
$$
\left\| \nabla f(x) - \nabla f(y)\right\|_2 \leq L \left\|x-y\right\|_2, \quad \forall x, y
$$
when the stepsize $t \leq 1/L$, we have 
$$
f(x^{(k)}) - f^* \le \frac{\|x^{(0)} - x^*\|^2}{2tk}
$$
indicating a convergence rate $\mathcal O(1/k)$. 
In another word, to achieve precision $\epsilon$, needs about $\mathcal O(1/\epsilon)$ times of iteration.

\subsubsection{Descent Lemma}
Express the difference between two points
$$f(\bm{y}) = f(\bm{x}) + \int_{0}^{1} \nabla f(\bm{x} + \tau(\bm{y} - \bm{x}))^T (\bm{y} - \bm{x}) \, d\tau$$
then introduce the gradient at $\bm x$
$$f(\bm{y}) = f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \int_{0}^{1} \left( \nabla f(\bm{x} + \tau(\bm{y} - \bm{x})) - \nabla f(\bm{x}) \right)^T (\bm{y} - \bm{x}) \, d\tau$$
apply Cauchy-Schwarz Inequality,
$$f(\bm{y}) \le f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \int_{0}^{1} \|\nabla f(\bm{x} + \tau(\bm{y} - \bm{x})) - \nabla f(\bm{x})\| \cdot \|\bm{y} - \bm{x}\| \, d\tau$$
Using the $L$-Lipschitz property, we know $\|\nabla f(\bm{x} + \tau(\bm{y} - \bm{x})) - \nabla f(\bm{x})\| \le L \|\tau(\bm{y} - \bm{x})\| = L\tau \|\bm{y} - \bm{x}\|$. 
Substituting this back:
\begin{align*}
    f(\bm{y}) &\le f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \int_{0}^{1} L\tau \|\bm{y} - \bm{x}\|^2 \, d\tau\\
    &= f(\bm{x}) + \nabla f(\bm{x})^T (\bm{y} - \bm{x}) + \frac{1}{2} L \|\bm{y} - \bm{x}\|^2 
\end{align*}

\subsubsection{Convergence rate}
In gradient descent update $\bm{x}^{(k+1)} = \bm{x}^{(k)} - t\nabla f(\bm{x}^{(k)})$ with step size $t \le 1/L$:
\begin{align*}
    f(\bm{x}^{(k+1)}) &\le f(\bm{x}^{(k)}) + \nabla f(\bm{x}^{(k)})^T (\bm{x}^{(k+1)} - \bm{x}^{(k)}) + \frac{L}{2} \|\bm{x}^{(k+1)} - \bm{x}^{(k)}\|^2\\
    &= f(\bm{x}^{(k)}) - t\|\nabla f(\bm{x}^{(k)})\|^2 + \frac{Lt^2}{2}\|\nabla f(\bm{x}^{(k)})\|^2\\
    &= f(\bm{x}^{(k)}) + (\frac{Lt^2}{2} - t)\|\nabla f(\bm{x}^{(k)})\|^2
\end{align*}
Since $t \le 1/L$, we have $-t + \frac{Lt^2}{2} \le -\frac{t}{2}$. Thus:
$$f(\bm{x}^{(k+1)}) \le f(\bm{x}^{(k)}) - \frac{t}{2}\|\nabla f(\bm{x}^{(k)})\|^2$$
which ensures \textbf{descent}.

By the convexity of $f$, we have $f(\bm{x}^*) \ge f(\bm{x}^{(k)}) + \nabla f(\bm{x}^{(k)})^T (\bm{x}^* - \bm{x}^{(k)})$, which rearranges to:
$$f(\bm{x}^{(k)}) - f^* \le \nabla f(\bm{x}^{(k)})^T (\bm{x}^{(k)} - \bm{x}^*)$$
plus the previous one, and get
$$f(\bm{x}^{(k+1)}) - f^* \le \nabla f(\bm{x}^{(k)})^T (\bm{x}^{(k)} - \bm{x}^*) - \frac{t}{2}\|\nabla f(\bm{x}^{(k)})\|^2$$

What's next? We want to know the relationship between the descent of $f(\bm x^{(k)}-f^*)$, and the descent of $\bm{x}^{(k)} - \bm{x}^*$, so consider
\begin{align*}
    \|\bm{x}^{(k+1)} - \bm{x}^*\|^2 &= \|(\bm{x}^{(k)} - \bm{x}^*) - t\nabla f(\bm{x}^{(k)})\|^2\\
    &= \|\bm{x}^{(k)} - \bm{x}^*\|^2 - 2t \nabla f(\bm{x}^{(k)})^T (\bm{x}^{(k)} - \bm{x}^*) + t^2 \|\nabla f(\bm{x}^{(k)})\|^2
\end{align*}
thus, 
$$f(\bm{x}^{(k+1)}) - f^* \le \frac{1}{2t} \left( \|\bm{x}^{(k)} - \bm{x}^*\|^2 - \|\bm{x}^{(k+1)} - \bm{x}^*\|^2 \right)$$
Summing from $i=0$ to $k-1$:
$$\sum_{i=0}^{k-1} (f(\bm{x}^{(i+1)}) - f^*) \le \frac{1}{2t} \left( \|\bm{x}^{(0)} - \bm{x}^*\|^2 - \|\bm{x}^{(k)} - \bm{x}^*\|^2 \right) \le \frac{\|\bm{x}^{(0)} - \bm{x}^*\|^2}{2t}$$
and we know $k(f(\bm{x}^{(k)}) - f^*) \le \sum_{i=0}^{k-1} (f(\bm{x}^{(i+1)}) - f^*)$, we get
$$f(\bm{x}^{(k)}) - f^* \le \frac{\|\bm{x}^{(0)} - \bm{x}^*\|^2}{2tk}$$


\subsubsection{Another view of GD}
If we approximate the function around $\bm x$ by replacing the Hessian $\nabla^2 f(\bm{x})$ with $\frac{1}{t}\bm{I}$:
$$\bm{x}^+ = \arg\min_{\bm{z}} \left( f(\bm{x}) + \nabla f(\bm{x})^T (\bm{z} - \bm{x}) + \frac{1}{2t} \|\bm{z} - \bm{x}\|_2^2 \right)$$
where $t$ is the step size. Surprisingly, taking the derivative of the right-hand side with respect to $\bm{z}$ and setting it to zero gives the gradient descent update:
$$\nabla f(\bm{x}) + \frac{1}{t}(\bm{z} - \bm{x}) = 0 \implies \bm{x}^+ = \bm{x} - t\nabla f(\bm{x})$$

From this view, the standard gradient descent assume the Hessian matrix is same.




\subsection{Subgradient descent}
Here we assume function is $G$ Lipschitz continuous:  $|f(\bm{x}) - f(\bm{y})| \le G\|\bm{x}-\bm{y}\|_2$, which bounds the norm of subgradient $\|\bm{g}\|_2 \le G$.

The subgradient update rule is $\bm{x}^{(k)} = \bm{x}^{(k-1)} - t_k \bm{g}^{(k-1)}$, where $\bm{g}^{(k-1)} \in \partial f(\bm{x}^{(k-1)})$.


Analyze the squared distance to the optimal point $\bm{x}^*$:
\begin{align*}
    \|\bm{x}^{(k)} - \bm{x}^*\|_2^2 &= \|\bm{x}^{(k-1)} - t_k \bm{g}^{(k-1)} - \bm{x}^*\|_2^2\\
& = \|\bm{x}^{(k-1)} - \bm{x}^*\|_2^2 - 2t_k (\bm{g}^{(k-1)})^T (\bm{x}^{(k-1)} - \bm{x}^*) + t_k^2 \|\bm{g}^{(k-1)}\|_2^2  
\end{align*}

From the definition of a subgradient: $(\bm{g}^{(k-1)})^T (\bm{x}^{(k-1)} - \bm{x}^*) \ge f(\bm{x}^{(k-1)}) - f(\bm{x}^*)$. 
Substituting this and $\|\bm{g}\|_2 \le G$:
\begin{equation*}
    \|\bm{x}^{(k)} - \bm{x}^*\|_2^2 \le \|\bm{x}^{(k-1)} - \bm{x}^*\|_2^2 - 2t_k (f(\bm{x}^{(k-1)}) - f^*) + t_k^2 G^2 \tag{$\ast$} \label{eq:descent}
\end{equation*}



Summing from $i=1$ to $k$ and noting $\|\bm{x}^{(k)} - \bm{x}^*\|_2^2 \ge 0$:
$$0 \le R^2 - 2\sum_{i=1}^k t_i (f(\bm{x}^{(i-1)}) - f^*) + G^2 \sum_{i=1}^k t_i^2$$
where $R = \|\bm{x}^{(0)} - \bm{x}^*\|_2$. 
Rearranging for the best iterate $f(\bm{x}_{best}^{(k)}) = \min_{i=0,\dots,k} f(\bm{x}^{(i)})$:
$$f(\bm{x}_{best}^{(k)}) - f^* \le \frac{R^2 + G^2 \sum_{i=1}^k t_i^2}{2 \sum_{i=1}^k t_i}$$

Thus, if $\sum_{i=1}^k t_i^2 < \infty$ and $\sum_{i=1}^k t_i = \infty$, subgradient can converge to optimal. 

For a fixed step size $t_i = t$:
$$f(\bm{x}_{best}^{(k)}) - f^* \le \frac{R^2}{2kt} + \frac{G^2 t}{2}$$
To achieve an error of $\epsilon$, we set $t = \epsilon/G^2$. 
This requires:
$$k \ge \frac{R^2 G^2}{\epsilon^2}$$
Thus, the subgradient method has a convergence rate of $\mathcal O(1/\epsilon^2)$, which is slower than the $\mathcal O(1/\epsilon)$ rate of gradient descent.

When optimal value $f^*$ is known, minimizing equotion \eqref{eq:descent} get Polyak step sizes:
$$
t_k = \frac{f(\bm{x}^{(k-1)}) - f^*}{\|\bm{g}^{(k-1)}\|_2^2}
$$


\subsection{Proximal Gradient Descent}
Although subgradient methods can solve non-differentiable optimization problems,
it's slow $\mathcal O(1/\epsilon^2)$. Thus, we try write it as a composite function
$$
f(\bm x) = g(\bm x) + h(\bm x)
$$
where $g$ is convex and differentiable, $h$ is convex but not necessarily differentiable.

For smooth $g$, use quadratic approximation, and for non-smooth $h$, just keep it.
\begin{align*}
    \bm{x}^{+} &= \underset{\bm{z}}{\text{argmin}} \left( g(\bm{x}) + \nabla g(\bm{x})^T(\bm{z}-\bm{x}) + \frac{1}{2t}\|\bm{z}-\bm{x}\|_2^2 \right) + h(\bm{z}) \\
        & =\underset{\bm{z}}{\text{argmin}} \left( \frac{1}{2t} \| \bm{z} - (\bm{x} - t \nabla g(\bm{x})) \|_2^2 + h(\bm{z}) \right)
\end{align*}
The first term make $\bm z$ stay close to the gradient update of $g$, while also make $h$ small (second term).

Define \textbf{proximal mapping}:
$$ \text{prox}_{h, t}(\bm{v}) = \underset{\bm{z}}{\text{argmin}} \left( \frac{1}{2t} \|\bm{z} - \bm{v}\|_2^2 + h(\bm{z}) \right) $$
then the procedure of proximal gradient descent is, for $k = 1, 2, 3, \dots$,
$$ \bm{v} = \bm{x}^{(k-1)} - t_k \nabla g(\bm{x}^{(k-1)}) $$
$$ \bm{x}^{(k)} = \text{prox}_{h, t_k}(\bm{v}) $$

\subsubsection{ISTA}\label{ISTA}
Iterative Shrinkage-Thresholding Algorithm (ISTA) is used for Lasso \ref{LASSO}.
$$\min \frac{1}{2}\|\bm{y} - X\bm{\beta}\|_2^2 + \lambda \|\bm{\beta}\|_1$$
$$ \text{prox}_{\lambda \|\cdot\|_1, t}(\bm{v}) = \underset{\bm{z}}{\text{argmin}} \left( \frac{1}{2t}\|\bm{z}-\bm{v}\|_2^2 + \lambda \|\bm{z}\|_1 \right) $$
The solution is soft-thresholding operator, denote $S_{\lambda t}(\bm{v})$
$$ [S_{\lambda t}(\bm{v})]_i = \begin{cases} v_i - \lambda t & \text{if } v_i > \lambda t \\ 0 & \text{if } |v_i| \le \lambda t \\ v_i + \lambda t & \text{if } v_i < -\lambda t \end{cases} $$

In practice, iteratively do
\begin{itemize}
    \item Residual gradient update: $\bm{v} = \bm{\beta} + t X^T(\bm{y} - X\bm{\beta})$
    \item Soft-thresholding truncate: $\bm{\beta}^{+} = S_{\lambda t}(\bm{v})$
\end{itemize}



\subsection{Stochastic gradient descent}







\newpage

\printbibliography

\end{document}
