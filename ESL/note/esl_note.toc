\contentsline {section}{\numberline {1}Intro}{3}{section.1}%
\contentsline {section}{\numberline {2}Chap 3: Linear Methods for Regression}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}Confidence region for $\hat {\beta }$}{4}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}What is Linear}{4}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Gauss-Markov Theorem}{4}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}QR decomposition}{5}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Application to Least Squared}{5}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}About orthogonal matrix}{6}{subsubsection.2.4.2}%
\contentsline {paragraph}{Computation of \(\bm {Q}^{\top }\bm {Q}\).}{6}{paragraph*.2}%
\contentsline {paragraph}{Computation of \(\bm {Q}\bm {Q}^{\top }\).}{6}{paragraph*.3}%
\contentsline {subsubsection}{\numberline {2.4.3}SVD and orthogonal matrix}{7}{subsubsection.2.4.3}%
\contentsline {subsection}{\numberline {2.5}Multiple testing in forward selection}{8}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Ridge regression}{8}{subsection.2.6}%
\contentsline {subsubsection}{\numberline {2.6.1}How to use ridge regression}{8}{subsubsection.2.6.1}%
\contentsline {subsubsection}{\numberline {2.6.2}Equivalence of two forms}{8}{subsubsection.2.6.2}%
\contentsline {subsubsection}{\numberline {2.6.3}Bayesian view}{10}{subsubsection.2.6.3}%
\contentsline {subsubsection}{\numberline {2.6.4}SVD in ridge}{10}{subsubsection.2.6.4}%
\contentsline {subsubsection}{\numberline {2.6.5}Revisit PCA}{11}{subsubsection.2.6.5}%
\contentsline {subsection}{\numberline {2.7}LASSO}{12}{subsection.2.7}%
\contentsline {subsubsection}{\numberline {2.7.1}Subgradient and subdifferential}{12}{subsubsection.2.7.1}%
\contentsline {subsubsection}{\numberline {2.7.2}Why shrink to zero}{13}{subsubsection.2.7.2}%
\contentsline {subsubsection}{\numberline {2.7.3}Soft thresholding}{14}{subsubsection.2.7.3}%
\contentsline {section}{\numberline {3}Chap 4: Linear methods for classification}{15}{section.3}%
\contentsline {subsection}{\numberline {3.1}Linear Regression of an Indicator Matrix}{15}{subsection.3.1}%
\contentsline {section}{\numberline {4}Optimization methods}{17}{section.4}%
\contentsline {subsection}{\numberline {4.1}Norms of matrices}{17}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Induced norm}{17}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Frobenius norm}{18}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Schatten norm}{19}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Gradient descent}{19}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Descent Lemma}{19}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Convergence rate}{20}{subsubsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.3}Another view of GD}{20}{subsubsection.4.2.3}%
\contentsline {subsection}{\numberline {4.3}Subgradient descent}{21}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Proximal Gradient Descent}{21}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}ISTA}{22}{subsubsection.4.4.1}%
\contentsline {subsection}{\numberline {4.5}Stochastic gradient descent}{22}{subsection.4.5}%
